{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### `---------------Mandatory Information to fill------------`"
      ],
      "metadata": {
        "id": "j7jO_ata_tGB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BJ7jLz7afs"
      },
      "source": [
        "### Group ID:\n",
        "### Group Members Name with Student ID:\n",
        "1. Student 1\n",
        "2. Student 2\n",
        "3. Student 3\n",
        "4. Student 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`-------------------Write your remarks (if any) that you want should get consider at the time of evaluation---------------`"
      ],
      "metadata": {
        "id": "YXHhoNgkAhUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarks: ##Add here"
      ],
      "metadata": {
        "id": "-5tK16CbA5X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Develop a reinforcement learning agent using dynamic programming methods to solve the Dice game optimally. The agent will learn the optimal policy by iteratively evaluating and improving its strategy based on the state-value function and the Bellman equations."
      ],
      "metadata": {
        "id": "7Uh03PdwA9-T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utCCy98e7aft"
      },
      "source": [
        "# Scenario:\n",
        "A player rolls a 6-sided die with the objective of reaching a score of **exactly** 100. On each turn, the player can choose to stop and keep their current score or continue rolling the die. If the player rolls a 1, they lose all points accumulated in that turn and the turn ends. If the player rolls any other number (2-6), that number is added to their score for that turn. The game ends when the player decides to stop and keep their score OR when the player's score reaches 100. The player wins if they reach a score of exactly 100, and loses if they roll a 1 when their score is below 100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Details\n",
        "\n",
        "* The environment consists of a player who can choose to either roll a 6-sided die or stop at any point.\n",
        "* The player starts with an initial score (e.g., 0) and aims to reach a score of exactly 100.\n",
        "* If the player rolls a 1, they lose all points accumulated in that turn and the turn ends. If they roll any other number (2-6), that number is added to their score for that turn.\n",
        "* The goal is to accumulate a total of exactly 100 points to win, or to stop the game before reaching 100 points.\n",
        "\n",
        "#### States\n",
        "* State s: Represents the current score of the player, ranging from 0 to 100.\n",
        "* Terminal States:\n",
        "    * State s = 100: Represents the player winning the game by reaching the goal of 100 points.\n",
        "    * State s = 0: Represents the player losing all points accumulated in the turn due to rolling a 1.\n",
        "\n",
        "\n",
        "#### Actions\n",
        "* Action a: Represents the decision to either \"roll\" the die or \"stop\" the game at the current score.\n",
        "* The possible actions in any state s are either \"roll\" or \"stop\".\n"
      ],
      "metadata": {
        "id": "9tvyyPpNB8M5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "851webtI7aft"
      },
      "source": [
        "# Expected Outcomes:\n",
        "1.\tUse dynamic programming methods value iteration, policy improvement and policy evaluation to find the optimal policy for the Dice Game.\n",
        "2.\tImplement an epsilon-greedy policy for action selection during training to balance exploration and exploitation.\n",
        "3.\tEvaluate the agent's performance in terms of the probability of reaching exactly 100 points after learning the optimal policy.\n",
        "4.\tUse the agent's policy as the best strategy for different betting scenarios within the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Execution"
      ],
      "metadata": {
        "id": "8GRKO9n6EeWe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxpP5E667afu"
      },
      "source": [
        "### Initialize constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewHMwHxk7afu"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "goal = 100\n",
        "gamma = 1.0\n",
        "prob_roll =\n",
        "\n",
        "# Initialize value function and policy\n",
        "V = np.zeros(goal + 1)\n",
        "policy = np.zeros(goal + 1, dtype=int)  # 0 for \"stop\", 1 for \"roll\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design a DiceGame Environment (1M)"
      ],
      "metadata": {
        "id": "HroEzPwhQkwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Dataset loading and print dataset statistics along with reward function\n",
        "#-----write your code below this line---------\n",
        "\n",
        "class DiceGameEnvironment:\n"
      ],
      "metadata": {
        "id": "Uc7EP7ZXQsn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0RcrK3U7afu"
      },
      "source": [
        "### Define reward funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwu1IhRZ7afv"
      },
      "outputs": [],
      "source": [
        "#Calculate reward function for 'stop' and 'roll' actions\n",
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KxOlvFZ7afv"
      },
      "source": [
        "# Policy Iteration Function Definition (0.5M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us3gUgp57afv"
      },
      "outputs": [],
      "source": [
        "#For each state, Store old_policy of state s.\n",
        "#Determine best_action based on maximum reward. Update policy[s] to best_action.\n",
        "#Return stable when old policy = policy[s]\n",
        "\n",
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sif2YH-B7afv"
      },
      "source": [
        "# Value Iteration Function Definition (0.5M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOlu2Jzl7afv"
      },
      "outputs": [],
      "source": [
        "# Iterate over all states except terminal state untill convergence\n",
        "\n",
        "# Calculate expected returns V(s) for current policy by considering all possible actions.\n",
        "\n",
        "#If action is stop:\n",
        "        #Calculate reward for stopping and append to rewards.\n",
        "#If action is roll:\n",
        "        #For each possible roll outcome (1 to 6), Determine next_s based on roll.\n",
        "\n",
        "# Update V(s) using the Bellman equation.\n",
        "\n",
        "#Determine max_reward from rewards\n",
        "#With probability epsilon, randomly choose a reward from rewards.\n",
        "\n",
        "#Check convergence if delta is less than a small threshold.\n",
        "\n",
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5flz-WD7afw"
      },
      "source": [
        "# Executing Policy Iteration and Value Iteration Functions (1M)\n",
        "\n",
        "Print all the iterations for both Policy and Value Iteration approaches separately. (Mandatory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMgBw1G77afw"
      },
      "outputs": [],
      "source": [
        "#Simulate the game for 100 states. Use the learned policy to get the actions.\n",
        "#when its roll, randomly generate a number to find the reward.\n",
        "#when its stop, get the respective reward\n",
        "#determine the total cumulative reward\n",
        "\n",
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaaPfNNa7afw"
      },
      "source": [
        "### Print the Learned Optimal Policy, Optimal Value Function (0.5M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BisSE1d27afw"
      },
      "outputs": [],
      "source": [
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGUqPYqF7afw"
      },
      "source": [
        "# Change in environment details (1M)\n",
        "\n",
        "Consider the following scenario:\n",
        "1. What happens if we change the goal score to 50 instead of 100? How does it affect the optimal policy and value function?\n",
        "2. How would the optimal policy and value function change if the die had 8 sides instead of 6? Assume the outcomes range from 0 to 7, with each outcome having a probability of 1/8.\n",
        "3. Experiment with different discount factors (e.g., 0.9, 0.95). How does discounting future rewards impact the optimal policy and value function?\n",
        "4. Create a heatmap or line plot to visualize the value function over different states. How does the value function change as the state approaches the goal?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf81q887afx"
      },
      "outputs": [],
      "source": [
        "#-----write your code below this line---------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion (0.5M)\n",
        "\n",
        "Conclude your assignment in 250 wrods by discussing the best approach for dice problem with the initial parameters and after chnaging the parameters.\n",
        "\n",
        "`----write below this line------`"
      ],
      "metadata": {
        "id": "xOQk0LcUImEW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}